{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0d1f00-4237-4bc8-89e5-71a0f812f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oplab\\anaconda3\\envs\\thyroid\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "C:\\Users\\oplab\\anaconda3\\envs\\thyroid\\Lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Dict\n",
    "import itertools\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import to_2tuple\n",
    "# 26m\n",
    "expansion_ratios_L = {\n",
    "    '0': [4, 4, 4, 4, 4],\n",
    "    '1': [4, 4, 4, 4, 4],\n",
    "    '2': [4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    '3': [4, 4, 4, 3, 3, 3, 3, 4, 4, 4],\n",
    "}\n",
    "\n",
    "expansion_ratios_L_reverse = {\n",
    "    '3': [4, 4, 4, 4, 4],\n",
    "    '2': [4, 4, 4, 4, 4],\n",
    "    '1': [4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4],\n",
    "    '0': [4, 4, 4, 3, 3, 3, 3, 4, 4, 4],\n",
    "}\n",
    "\n",
    "class Attention4D(torch.nn.Module):\n",
    "    def __init__(self, dim=384, key_dim=32, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=7,\n",
    "                 act_layer=nn.ReLU,\n",
    "                 stride=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.nh_kd = nh_kd = key_dim * num_heads\n",
    "\n",
    "        if stride is not None:\n",
    "            self.resolution = math.ceil(resolution / stride)\n",
    "            self.stride_conv = nn.Sequential(nn.Conv2d(dim, dim, kernel_size=3, stride=stride, padding=1, groups=dim),\n",
    "                                             nn.BatchNorm2d(dim), )\n",
    "            self.upsample = nn.Upsample(scale_factor=stride, mode='bilinear')\n",
    "        else:\n",
    "            self.resolution = resolution\n",
    "            self.stride_conv = None\n",
    "            self.upsample = None\n",
    "\n",
    "        self.N = self.resolution ** 2\n",
    "        self.N2 = self.N\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.dh = int(attn_ratio * key_dim) * num_heads\n",
    "        self.attn_ratio = attn_ratio\n",
    "        h = self.dh + nh_kd * 2\n",
    "        self.q = nn.Sequential(nn.Conv2d(dim, self.num_heads * self.key_dim, 1),\n",
    "                               nn.BatchNorm2d(self.num_heads * self.key_dim), )\n",
    "        self.k = nn.Sequential(nn.Conv2d(dim, self.num_heads * self.key_dim, 1),\n",
    "                               nn.BatchNorm2d(self.num_heads * self.key_dim), )\n",
    "        self.v = nn.Sequential(nn.Conv2d(dim, self.num_heads * self.d, 1),\n",
    "                               nn.BatchNorm2d(self.num_heads * self.d),\n",
    "                               )\n",
    "        self.v_local = nn.Sequential(nn.Conv2d(self.num_heads * self.d, self.num_heads * self.d,\n",
    "                                               kernel_size=3, stride=1, padding=1, groups=self.num_heads * self.d),\n",
    "                                     nn.BatchNorm2d(self.num_heads * self.d), )\n",
    "        self.talking_head1 = nn.Conv2d(self.num_heads, self.num_heads, kernel_size=1, stride=1, padding=0)\n",
    "        self.talking_head2 = nn.Conv2d(self.num_heads, self.num_heads, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.proj = nn.Sequential(act_layer(),\n",
    "                                  nn.Conv2d(self.dh, dim, 1),\n",
    "                                  nn.BatchNorm2d(dim), )\n",
    "\n",
    "        points = list(itertools.product(range(self.resolution), range(self.resolution)))\n",
    "        N = len(points)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "        for p1 in points:\n",
    "            for p2 in points:\n",
    "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                idxs.append(attention_offsets[offset])\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_heads, len(attention_offsets)))\n",
    "        self.register_buffer('attention_bias_idxs',\n",
    "                             torch.LongTensor(idxs).view(N, N))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and hasattr(self, 'ab'):\n",
    "            del self.ab\n",
    "        else:\n",
    "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "\n",
    "    def forward(self, x):  # x (B,N,C)\n",
    "        B, C, H, W = x.shape\n",
    "        if self.stride_conv is not None:\n",
    "            x = self.stride_conv(x)\n",
    "\n",
    "        q = self.q(x).flatten(2).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 3, 2)\n",
    "        k = self.k(x).flatten(2).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 2, 3)\n",
    "        v = self.v(x)\n",
    "        v_local = self.v_local(v)\n",
    "        v = v.flatten(2).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 3, 2)\n",
    "\n",
    "        attn = (\n",
    "                (q @ k) * self.scale\n",
    "                +\n",
    "                (self.attention_biases[:, self.attention_bias_idxs]\n",
    "                 if self.training else self.ab)\n",
    "        )\n",
    "        attn = self.talking_head1(attn)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.talking_head2(attn)\n",
    "\n",
    "        x = (attn @ v)\n",
    "\n",
    "        out = x.transpose(2, 3).reshape(B, self.dh, self.resolution, self.resolution) + v_local\n",
    "        if self.upsample is not None:\n",
    "            out = self.upsample(out)\n",
    "\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def stem(in_chs, out_chs, act_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_chs, out_chs // 2, kernel_size=3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(out_chs // 2),\n",
    "        act_layer(),\n",
    "        nn.Conv2d(out_chs // 2, out_chs, kernel_size=3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(out_chs),\n",
    "        act_layer(),\n",
    "    )\n",
    "def merge(in_chs, out_chs, act_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_chs, out_chs // 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(out_chs // 2),\n",
    "        act_layer(),\n",
    "        nn.ConvTranspose2d(out_chs // 2, out_chs, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(out_chs),\n",
    "        act_layer(),\n",
    "    )\n",
    "\n",
    "class LGQuery(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, resolution1, resolution2):\n",
    "        super().__init__()\n",
    "        self.resolution1 = resolution1\n",
    "        self.resolution2 = resolution2\n",
    "        self.pool = nn.AvgPool2d(1, 2, 0)\n",
    "        self.local = nn.Sequential(nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=2, padding=1, groups=in_dim),\n",
    "                                   )\n",
    "        self.proj = nn.Sequential(nn.Conv2d(in_dim, out_dim, 1),\n",
    "                                  nn.BatchNorm2d(out_dim), )\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_q = self.local(x)\n",
    "        pool_q = self.pool(x)\n",
    "        q = local_q + pool_q\n",
    "        q = self.proj(q)\n",
    "        return q\n",
    "    \n",
    "class UpLGQuery(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, resolution1, resolution2):\n",
    "        super().__init__()\n",
    "        self.resolution1 = resolution1\n",
    "        self.resolution2 = resolution2\n",
    "        self.pool = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  \n",
    "        self.local = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_dim, in_dim, kernel_size=3, stride=2, padding=1, output_padding=1, groups=in_dim),\n",
    "        )\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, 1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_q = self.local(x)\n",
    "        pool_q = self.pool(x)\n",
    "        q = local_q + pool_q\n",
    "        q = self.proj(q)\n",
    "        return q\n",
    "\n",
    "\n",
    "class Attention4DDownsample(torch.nn.Module):\n",
    "    def __init__(self, dim=384, key_dim=16, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=7,\n",
    "                 out_dim=None,\n",
    "                 act_layer=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.nh_kd = nh_kd = key_dim * num_heads\n",
    "\n",
    "        self.resolution = resolution\n",
    "\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.dh = int(attn_ratio * key_dim) * num_heads\n",
    "        self.attn_ratio = attn_ratio\n",
    "        h = self.dh + nh_kd * 2\n",
    "\n",
    "        if out_dim is not None:\n",
    "            self.out_dim = out_dim\n",
    "        else:\n",
    "            self.out_dim = dim\n",
    "        self.resolution2 = math.ceil(self.resolution / 2)\n",
    "        self.q = LGQuery(dim, self.num_heads * self.key_dim, self.resolution, self.resolution2)\n",
    "\n",
    "        self.N = self.resolution ** 2\n",
    "        self.N2 = self.resolution2 ** 2\n",
    "\n",
    "        self.k = nn.Sequential(nn.Conv2d(dim, self.num_heads * self.key_dim, 1),\n",
    "                               nn.BatchNorm2d(self.num_heads * self.key_dim), )\n",
    "        self.v = nn.Sequential(nn.Conv2d(dim, self.num_heads * self.d, 1),\n",
    "                               nn.BatchNorm2d(self.num_heads * self.d),\n",
    "                               )\n",
    "        self.v_local = nn.Sequential(nn.Conv2d(self.num_heads * self.d, self.num_heads * self.d,\n",
    "                                               kernel_size=3, stride=2, padding=1, groups=self.num_heads * self.d),\n",
    "                                     nn.BatchNorm2d(self.num_heads * self.d), )\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            act_layer(),\n",
    "            nn.Conv2d(self.dh, self.out_dim, 1),\n",
    "            nn.BatchNorm2d(self.out_dim), )\n",
    "\n",
    "        points = list(itertools.product(range(self.resolution), range(self.resolution)))\n",
    "        points_ = list(itertools.product(\n",
    "            range(self.resolution2), range(self.resolution2)))\n",
    "        N = len(points)\n",
    "        N_ = len(points_)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "        for p1 in points_:\n",
    "            for p2 in points:\n",
    "                size = 1\n",
    "                offset = (\n",
    "                    abs(p1[0] * math.ceil(self.resolution / self.resolution2) - p2[0] + (size - 1) / 2),\n",
    "                    abs(p1[1] * math.ceil(self.resolution / self.resolution2) - p2[1] + (size - 1) / 2))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                idxs.append(attention_offsets[offset])\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_heads, len(attention_offsets)))\n",
    "        self.register_buffer('attention_bias_idxs',\n",
    "                             torch.LongTensor(idxs).view(N_, N))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and hasattr(self, 'ab'):\n",
    "            del self.ab\n",
    "        else:\n",
    "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "\n",
    "    def forward(self, x):  # x (B,N,C)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        q = self.q(x).flatten(2).reshape(B, self.num_heads, -1, self.N2).permute(0, 1, 3, 2)\n",
    "        k = self.k(x).flatten(2).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 2, 3)\n",
    "        v = self.v(x)\n",
    "        v_local = self.v_local(v)\n",
    "        v = v.flatten(2).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 3, 2)\n",
    "\n",
    "        attn = (\n",
    "                (q @ k) * self.scale\n",
    "                +\n",
    "                (self.attention_biases[:, self.attention_bias_idxs]\n",
    "                 if self.training else self.ab)\n",
    "        )\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(2, 3)\n",
    "        out = x.reshape(B, self.dh, self.resolution2, self.resolution2) + v_local\n",
    "\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class Attention4DUpsample(nn.Module):\n",
    "    def __init__(self, dim=384, key_dim=16, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=7,\n",
    "                 resolution2=7,\n",
    "                 out_dim=None,\n",
    "                 act_layer=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.nh_kd = nh_kd = key_dim * num_heads\n",
    "\n",
    "        self.resolution = resolution\n",
    "\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.dh = int(attn_ratio * key_dim) * num_heads\n",
    "        self.attn_ratio = attn_ratio\n",
    "        h = self.dh + nh_kd * 2\n",
    "\n",
    "        if out_dim is not None:\n",
    "            self.out_dim = out_dim\n",
    "        else:\n",
    "            self.out_dim = dim\n",
    "        self.resolution2 = resolution2  \n",
    "        self.q = UpLGQuery(dim, self.num_heads * self.key_dim, self.resolution, self.resolution2)\n",
    "\n",
    "        self.N = self.resolution ** 2\n",
    "        self.N2 = self.resolution2 ** 2\n",
    "\n",
    "        self.k = nn.Sequential(nn.Conv2d(dim, self.num_heads * self.key_dim, 1),\n",
    "                               nn.BatchNorm2d(self.num_heads * self.key_dim), )\n",
    "        self.v = nn.Sequential(nn.Conv2d(dim, self.num_heads * self.d, 1),\n",
    "                               nn.BatchNorm2d(self.num_heads * self.d),\n",
    "                               )\n",
    "        self.v_local = nn.Sequential(nn.ConvTranspose2d(self.num_heads * self.d, self.num_heads * self.d,\n",
    "                                               kernel_size=3, stride=2, padding=1, output_padding=1, groups=self.num_heads * self.d),\n",
    "                                     nn.BatchNorm2d(self.num_heads * self.d), )\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            act_layer(),\n",
    "            nn.Conv2d(self.dh, self.out_dim, 1),\n",
    "            nn.BatchNorm2d(self.out_dim), )\n",
    "\n",
    "        points = list(itertools.product(range(self.resolution), range(self.resolution)))\n",
    "        points_ = list(itertools.product(\n",
    "            range(self.resolution2), range(self.resolution2)))\n",
    "        N = len(points)\n",
    "        N_ = len(points_)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "\n",
    "    def forward(self, x):  # x (B,N,C)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        q = self.q(x).flatten(2).reshape(B, self.num_heads, -1, self.N2).permute(0, 1, 3, 2)\n",
    "        k = self.k(x).flatten(2).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 2, 3)\n",
    "        v = self.v(x)\n",
    "        v_local = self.v_local(v)\n",
    "        v = v.flatten(2).reshape(B, self.num_heads, -1, self.N).permute(0, 1, 3, 2)\n",
    "        \n",
    "\n",
    "        attn = (\n",
    "                (q @ k) * self.scale\n",
    "        )\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(2, 3)\n",
    "        out = x.reshape(B, self.dh, self.resolution2, self.resolution2) + v_local\n",
    "\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, patch_size=3, stride=2, padding=1,\n",
    "                 in_chans=3, embed_dim=768, norm_layer=nn.BatchNorm2d,\n",
    "                 light=False, asub=False, resolution=None, act_layer=nn.ReLU, attn_block=Attention4DDownsample):\n",
    "        super().__init__()\n",
    "        self.light = light\n",
    "        self.asub = asub\n",
    "\n",
    "        if self.light:\n",
    "            self.new_proj = nn.Sequential(\n",
    "                nn.Conv2d(in_chans, in_chans, kernel_size=3, stride=2, padding=1, groups=in_chans),\n",
    "                nn.BatchNorm2d(in_chans),\n",
    "                nn.Hardswish(),\n",
    "                nn.Conv2d(in_chans, embed_dim, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(embed_dim),\n",
    "            )\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_chans, embed_dim, kernel_size=1, stride=2, padding=0),\n",
    "                nn.BatchNorm2d(embed_dim)\n",
    "            )\n",
    "        elif self.asub:\n",
    "            self.attn = attn_block(dim=in_chans, out_dim=embed_dim,\n",
    "                                   resolution=resolution, act_layer=act_layer)\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            stride = to_2tuple(stride)\n",
    "            padding = to_2tuple(padding)\n",
    "            self.conv = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size,\n",
    "                                  stride=stride, padding=padding)\n",
    "            self.bn = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        else:\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            stride = to_2tuple(stride)\n",
    "            padding = to_2tuple(padding)\n",
    "            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size,\n",
    "                                  stride=stride, padding=padding)\n",
    "            self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.light:\n",
    "            out = self.new_proj(x) + self.skip(x)\n",
    "        elif self.asub:\n",
    "            out_conv = self.conv(x)\n",
    "            out_conv = self.bn(out_conv)\n",
    "            out = self.attn(x) + out_conv\n",
    "        else:\n",
    "            x = self.proj(x)\n",
    "            out = self.norm(x)\n",
    "        return out\n",
    "\n",
    "class Expanding(nn.Module):\n",
    "    def __init__(self, patch_size=2, stride=2, padding=1,\n",
    "                 in_chans=3, embed_dim=768, norm_layer=nn.BatchNorm2d,\n",
    "                 light=False, asub=False, resolution=None, resolution2=None, act_layer=nn.ReLU,\n",
    "                 attn_block=Attention4DUpsample):  \n",
    "        super().__init__()\n",
    "        self.light = light\n",
    "        self.asub = asub\n",
    "        \n",
    "        if self.light:\n",
    "            self.new_proj = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_chans, in_chans, kernel_size=3, stride=2, padding=1, output_padding=1, groups=in_chans),\n",
    "                nn.BatchNorm2d(in_chans),\n",
    "                nn.Hardswish(),\n",
    "                nn.Conv2d(in_chans, embed_dim, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(embed_dim),\n",
    "            )\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_chans, embed_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(embed_dim),\n",
    "            )\n",
    "        elif self.asub:\n",
    "            self.attn = attn_block(dim=in_chans, out_dim=embed_dim,\n",
    "                                   resolution=resolution, resolution2=resolution2, act_layer=act_layer)\n",
    "            patch_size = (patch_size, patch_size)\n",
    "            stride = (stride, stride)\n",
    "            padding = (padding, padding)\n",
    "            self.conv = nn.ConvTranspose2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=padding, output_padding=1)\n",
    "            self.bn = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        else:\n",
    "            patch_size = (patch_size, patch_size)\n",
    "            stride = (stride, stride)\n",
    "            padding = (padding, padding)\n",
    "            self.proj = nn.ConvTranspose2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=padding, output_padding=1)\n",
    "            self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.light:\n",
    "            out = self.new_proj(x) + self.skip(x)\n",
    "        elif self.asub:\n",
    "            out_conv = self.conv(x)\n",
    "            out_conv = self.bn(out_conv)\n",
    "            \n",
    "            out = self.attn(x) + out_conv\n",
    "        else:\n",
    "            x = self.proj(x)\n",
    "            out = self.norm(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of MLP with 1*1 convolutions.\n",
    "    Input: tensor with shape [B, C, H, W]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None,\n",
    "                 out_features=None, act_layer=nn.GELU, drop=0., mid_conv=False):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.mid_conv = mid_conv\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        if self.mid_conv:\n",
    "            self.mid = nn.Conv2d(hidden_features, hidden_features, kernel_size=3, stride=1, padding=1,\n",
    "                                 groups=hidden_features)\n",
    "            self.mid_norm = nn.BatchNorm2d(hidden_features)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(hidden_features)\n",
    "        self.norm2 = nn.BatchNorm2d(out_features)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        if self.mid_conv:\n",
    "            x_mid = self.mid(x)\n",
    "            x_mid = self.mid_norm(x_mid)\n",
    "            x = self.act(x_mid)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttnFFN(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4.,\n",
    "                 act_layer=nn.ReLU, norm_layer=nn.LayerNorm,\n",
    "                 drop=0., drop_path=0.,\n",
    "                 use_layer_scale=True, layer_scale_init_value=1e-5,\n",
    "                 resolution=7, stride=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_mixer = Attention4D(dim, resolution=resolution, act_layer=act_layer, stride=stride)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop, mid_conv=True)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. \\\n",
    "            else nn.Identity()\n",
    "        self.use_layer_scale = use_layer_scale\n",
    "        if use_layer_scale:\n",
    "            self.layer_scale_1 = nn.Parameter(\n",
    "                layer_scale_init_value * torch.ones(dim).unsqueeze(-1).unsqueeze(-1), requires_grad=True)\n",
    "            self.layer_scale_2 = nn.Parameter(\n",
    "                layer_scale_init_value * torch.ones(dim).unsqueeze(-1).unsqueeze(-1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_layer_scale:\n",
    "            x = x + self.drop_path(self.layer_scale_1 * self.token_mixer(x))\n",
    "            x = x + self.drop_path(self.layer_scale_2 * self.mlp(x))\n",
    "\n",
    "        else:\n",
    "            x = x + self.drop_path(self.token_mixer(x))\n",
    "            x = x + self.drop_path(self.mlp(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, pool_size=3, mlp_ratio=4.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 drop=0., drop_path=0.,\n",
    "                 use_layer_scale=True, layer_scale_init_value=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop, mid_conv=True)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. \\\n",
    "            else nn.Identity()\n",
    "        self.use_layer_scale = use_layer_scale\n",
    "        if use_layer_scale:\n",
    "            self.layer_scale_2 = nn.Parameter(\n",
    "                layer_scale_init_value * torch.ones(dim).unsqueeze(-1).unsqueeze(-1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_layer_scale:\n",
    "            x = x + self.drop_path(self.layer_scale_2 * self.mlp(x))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.mlp(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def eformer_block(dim, index, layers,\n",
    "                  pool_size=3, mlp_ratio=4.,\n",
    "                  act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                  drop_rate=.0, drop_path_rate=0.,\n",
    "                  use_layer_scale=True, layer_scale_init_value=1e-5, vit_num=1, resolution=7, e_ratios=None):\n",
    "    blocks = []\n",
    "    for block_idx in range(layers[index]):\n",
    "        block_dpr = drop_path_rate * (\n",
    "                block_idx + sum(layers[:index])) / (sum(layers) - 1)\n",
    "        mlp_ratio = e_ratios[str(index)][block_idx]\n",
    "        if index >= 2 and block_idx > layers[index] - 1 - vit_num:\n",
    "            if index == 2:\n",
    "                stride = 2\n",
    "            else:\n",
    "                stride = None\n",
    "            blocks.append(AttnFFN(\n",
    "                dim, mlp_ratio=mlp_ratio,\n",
    "                act_layer=act_layer, norm_layer=norm_layer,\n",
    "                drop=drop_rate, drop_path=block_dpr,\n",
    "                use_layer_scale=use_layer_scale,\n",
    "                layer_scale_init_value=layer_scale_init_value,\n",
    "                resolution=resolution,\n",
    "                stride=stride,\n",
    "            ))\n",
    "        else:\n",
    "            blocks.append(FFN(\n",
    "                dim, pool_size=pool_size, mlp_ratio=mlp_ratio,\n",
    "                act_layer=act_layer,\n",
    "                drop=drop_rate, drop_path=block_dpr,\n",
    "                use_layer_scale=use_layer_scale,\n",
    "                layer_scale_init_value=layer_scale_init_value,\n",
    "            ))\n",
    "    blocks = nn.Sequential(*blocks)\n",
    "    return blocks\n",
    "\n",
    "def eformer_block_up(dim, index, layers,\n",
    "                  pool_size=3, mlp_ratio=4.,\n",
    "                  act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                  drop_rate=.0, drop_path_rate=0.,\n",
    "                  use_layer_scale=True, layer_scale_init_value=1e-5, vit_num=1, resolution=7, e_ratios=None):\n",
    "    blocks = []\n",
    "    for block_idx in range(layers[index]):\n",
    "        block_dpr = drop_path_rate * (\n",
    "                block_idx + sum(layers[index+1:])) / (sum(layers) - 1)\n",
    "        mlp_ratio = e_ratios[str(index)][block_idx]\n",
    "        if index <= 1 and block_idx > layers[index] - 1 - vit_num:\n",
    "            if index == 1:\n",
    "                stride = 2\n",
    "            else:\n",
    "                stride = None\n",
    "            blocks.append(AttnFFN(\n",
    "                dim, mlp_ratio=mlp_ratio,\n",
    "                act_layer=act_layer, norm_layer=norm_layer,\n",
    "                drop=drop_rate, drop_path=block_dpr,\n",
    "                use_layer_scale=use_layer_scale,\n",
    "                layer_scale_init_value=layer_scale_init_value,\n",
    "                resolution=resolution,\n",
    "                stride=stride,\n",
    "            ))\n",
    "        else:\n",
    "            blocks.append(FFN(\n",
    "                dim, pool_size=pool_size, mlp_ratio=mlp_ratio,\n",
    "                act_layer=act_layer,\n",
    "                drop=drop_rate, drop_path=block_dpr,\n",
    "                use_layer_scale=use_layer_scale,\n",
    "                layer_scale_init_value=layer_scale_init_value,\n",
    "            ))\n",
    "    blocks = nn.Sequential(*blocks)\n",
    "    return blocks\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class CCA(nn.Module):\n",
    "    \"\"\"\n",
    "    CCA Block\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_x):\n",
    "        super().__init__()\n",
    "        self.mlp_x = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(F_x, F_x))\n",
    "        self.mlp_g = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(F_g, F_x))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # channel-wise attention\n",
    "        avg_pool_x = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "        channel_att_x = self.mlp_x(avg_pool_x)\n",
    "        avg_pool_g = F.avg_pool2d( g, (g.size(2), g.size(3)), stride=(g.size(2), g.size(3)))\n",
    "        channel_att_g = self.mlp_g(avg_pool_g)\n",
    "        channel_att_sum = (channel_att_x + channel_att_g)/2.0\n",
    "        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        x_after_channel = x * scale\n",
    "        out = self.relu(x_after_channel)\n",
    "        return out\n",
    "\n",
    "class Eff_Unet(nn.Module):\n",
    "    def __init__(self, layers, embed_dims=None,\n",
    "                 mlp_ratios=4, downsamples=None,\n",
    "                 pool_size=3,\n",
    "                 norm_layer=nn.BatchNorm2d, act_layer=nn.GELU,\n",
    "                 num_classes=9,\n",
    "                 down_patch_size=3, down_stride=2, down_pad=1,\n",
    "                 drop_rate=0., drop_path_rate=0.,\n",
    "                 use_layer_scale=True, layer_scale_init_value=1e-5,\n",
    "                 init_cfg=None,\n",
    "                 pretrained=None,\n",
    "                 vit_num=0,\n",
    "                 distillation=True,\n",
    "                 resolution=224,\n",
    "                 e_ratios=expansion_ratios_L,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_embed = stem(1, embed_dims[0], act_layer=act_layer)\n",
    "        self.up = merge(embed_dims[0],embed_dims[0], act_layer=act_layer)\n",
    "        self.output = nn.Conv2d(in_channels=embed_dims[0],out_channels=self.num_classes,kernel_size=1,bias=False)\n",
    "    \n",
    "        network_down = []\n",
    "        res_arr = []\n",
    "        if self.num_classes == 1:\n",
    "            self.sig = nn.Sigmoid()    \n",
    "\n",
    "\n",
    "        for i in range(len(layers)):\n",
    "            res=math.ceil(resolution / (2 ** (i + 2)))\n",
    "            res_arr.append(res)\n",
    "            stage = eformer_block(embed_dims[i], i, layers,\n",
    "                                  pool_size=pool_size, mlp_ratio=mlp_ratios,\n",
    "                                  act_layer=act_layer, norm_layer=norm_layer,\n",
    "                                  drop_rate=drop_rate,\n",
    "                                  drop_path_rate=drop_path_rate,\n",
    "                                  use_layer_scale=use_layer_scale,\n",
    "                                  layer_scale_init_value=layer_scale_init_value,\n",
    "                                  resolution=res,\n",
    "                                  vit_num=vit_num,\n",
    "                                  e_ratios=e_ratios)\n",
    "            network_down.append(stage)\n",
    "\n",
    "            if i >= len(layers) - 1:\n",
    "                break\n",
    "            if downsamples[i] or embed_dims[i] != embed_dims[i + 1]:\n",
    "                # downsampling between two stages\n",
    "                if i >= 2:\n",
    "                    asub = True\n",
    "                else:\n",
    "                    asub = False\n",
    "                network_down.append(\n",
    "                    Embedding(\n",
    "                        patch_size=down_patch_size, stride=down_stride,\n",
    "                        padding=down_pad,\n",
    "                        in_chans=embed_dims[i], embed_dim=embed_dims[i + 1],\n",
    "                        resolution=res,\n",
    "                        asub=asub,\n",
    "                        act_layer=act_layer, norm_layer=norm_layer,\n",
    "                    )\n",
    "                )\n",
    "        self.network_down_layers = nn.ModuleList(network_down)\n",
    "        \n",
    "        # build decoder layers\n",
    "        self.network_up_layers = nn.ModuleList()\n",
    "        self.concat_back_dim = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        self.skip_activation = nn.ReLU()\n",
    "        \n",
    "        self.coatt = nn.ModuleList()\n",
    "        layers_reverse = layers[::-1]\n",
    "        embed_dim_reverse = embed_dims[::-1] \n",
    "        res_arr = res_arr[::-1]\n",
    "        \n",
    "        self.embed_dim_reverse = embed_dim_reverse\n",
    "        for i in range(len(layers)):\n",
    "            layers_reverse = layers[::-1]\n",
    "            embed_dim_reverse = embed_dims[::-1] \n",
    "            skip_down_conv = nn.Conv2d(in_channels=2*embed_dim_reverse[i], out_channels=embed_dim_reverse[i], kernel_size=3, padding=1)\n",
    "            self.coatt.append(CCA(F_g=embed_dim_reverse[i], F_x=embed_dim_reverse[i]))\n",
    "\n",
    "            if i > 0 :\n",
    "                if i <= 2:\n",
    "                    asub = True\n",
    "                else:\n",
    "                    asub = False\n",
    "                self.network_up_layers.append(\n",
    "                    Expanding(\n",
    "                        patch_size=down_patch_size, stride=down_stride,\n",
    "                        padding=down_pad,\n",
    "                        in_chans=embed_dim_reverse[i-1], embed_dim=embed_dim_reverse[i],\n",
    "                        resolution=res_arr[i-1],\n",
    "                        resolution2 = res_arr[i],\n",
    "                        asub=asub,\n",
    "                        act_layer=act_layer, norm_layer=norm_layer,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            network_up = eformer_block_up(embed_dim_reverse[i], i, layers_reverse,\n",
    "                                  pool_size=pool_size, mlp_ratio=mlp_ratios,\n",
    "                                  act_layer=act_layer, norm_layer=norm_layer,\n",
    "                                  drop_rate=drop_rate,\n",
    "                                  drop_path_rate=drop_path_rate,\n",
    "                                  use_layer_scale=use_layer_scale,\n",
    "                                  layer_scale_init_value=layer_scale_init_value,\n",
    "                                  resolution=res_arr[i],\n",
    "                                  vit_num=vit_num,\n",
    "                                  e_ratios=expansion_ratios_L_reverse)\n",
    "                \n",
    "                \n",
    "            self.network_up_layers.append(network_up)\n",
    "            self.concat_back_dim.append(skip_down_conv)\n",
    "\n",
    "\n",
    "        self.out_indices = [0, 2, 4, 6]\n",
    "        for i_emb, i_layer in enumerate(self.out_indices):\n",
    "            if i_emb == 0 and os.environ.get('FORK_LAST3', None):\n",
    "                layer = nn.Identity()\n",
    "            else:\n",
    "                layer = norm_layer(embed_dims[i_emb])\n",
    "            layer_name = f'norm{i_layer}'\n",
    "            self.add_module(layer_name, layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # init for classification\n",
    "    def cls_init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # init for mmdetection or mmsegmentation by loading\n",
    "    # imagenet pre-trained weights\n",
    "\n",
    "    def forward_tokens(self, x):\n",
    "        outs = []\n",
    "        for idx, block in enumerate(self.network_down_layers):\n",
    "            x = block(x)\n",
    "            if idx in self.out_indices:\n",
    "\n",
    "                norm_layer = getattr(self, f'norm{idx}')\n",
    "                x_out = norm_layer(x)\n",
    "                outs.append(x_out)\n",
    "        return x, outs\n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x, x_downsample = self.forward_tokens(x)\n",
    "        # x = self.norm(x)\n",
    "        return x, x_downsample\n",
    "    \n",
    "    def forward_up_features(self,x,x_downsample):\n",
    "        skip_conn = [0, 2, 4, 6]\n",
    "        x_downsample_reverse = x_downsample[::-1]\n",
    "        for idx, block in enumerate(self.network_up_layers):\n",
    "            if idx not in skip_conn:\n",
    "                x = block(x)\n",
    "            else:        \n",
    "                if self.num_classes != 1:\n",
    "                    skip_x_att = self.coatt[skip_conn.index(idx)](x, x_downsample_reverse[skip_conn.index(idx)])\n",
    "                    x = torch.cat([x,skip_x_att],1)\n",
    "                    x = self.concat_back_dim[skip_conn.index(idx)](x)\n",
    "                    x = block(x)\n",
    "                else:                \n",
    "                    x = torch.cat([x,x_downsample_reverse[skip_conn.index(idx)]],1)\n",
    "                    x = self.concat_back_dim[skip_conn.index(idx)](x)\n",
    "                    x = block(x)\n",
    "        return x\n",
    "    \n",
    "    def up_sample(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x_downsample = self.forward_features(x)\n",
    "        x = self.forward_up_features(x, x_downsample)\n",
    "        x = self.up_sample(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        if self.num_classes == 1:\n",
    "            x = self.sig(x)\n",
    "        return x\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 9, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .95, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "def load_from(model, ckpt_path):\n",
    "    pretrained_path = ckpt_path\n",
    "    if pretrained_path is not None:\n",
    "        print(\"pretrained_path:{}\".format(pretrained_path))\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=device)\n",
    "\n",
    "        pretrained_dict = pretrained_dict['model']\n",
    "        model_dict = model.state_dict()                \n",
    "        full_dict = copy.deepcopy(pretrained_dict)\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if \"network.\" in k:\n",
    "                up_layer_num = 6-int(k.split('.')[1])\n",
    "                current_k_up = \"network_up_layers.\" + str(up_layer_num) + '.' + '.'.join(k.split('.')[2:])\n",
    "                full_dict.update({current_k_up:v})\n",
    "                full_dict[\"network_down_layers.\" + '.'.join(k.split('.')[1:])] = full_dict.pop(k)\n",
    "\n",
    "        for k in list(full_dict.keys()):\n",
    "            if k in model_dict:\n",
    "                if full_dict[k].shape != model_dict[k].shape:\n",
    "                    print(\"delete:{};shape pretrain:{};shape model:{}\".format(k,v.shape,model_dict[k].shape))\n",
    "                    del full_dict[k]\n",
    "\n",
    "\n",
    "        msg = model.load_state_dict(full_dict, strict=False)\n",
    "        print(\"pretrained weights are loaded\")\n",
    "        return model\n",
    "    else:\n",
    "        print(\"none pretrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c80a24-8a40-4281-a53e-a07b43edb569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5baf66-d5c5-461d-872e-9693d15fa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sime = 128\n",
    "input_example = torch.zeros((8, 1, image_sime, image_sime)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b2ad46-ad02-4b48-be92-0d52689dc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Eff_Unet(\n",
    "        layers=[5, 5, 15, 10],\n",
    "        embed_dims=[40, 80, 192, 384],\n",
    "        downsamples=[True, True, True, True],\n",
    "        vit_num=6,\n",
    "        drop_path_rate=0.1,\n",
    "        num_classes=9,\n",
    "        resolution = image_sime).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a461bfd-74b9-4b51-863a-dac96c7a39cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_example = net(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282b0a9-a0e4-4435-915a-adb3a99d84b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thyroid",
   "language": "python",
   "name": "thyroid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
