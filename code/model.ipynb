{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c0d1f00-4237-4bc8-89e5-71a0f812f791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1, bias=False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        groups = 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=kernel//2, groups=groups, bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU6(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "class CombConvLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel=1, stride=1, dropout=0.1, bias=False):\n",
    "        super().__init__()\n",
    "        self.add_module('layer1',ConvLayer(in_channels, out_channels, kernel))\n",
    "        self.add_module('layer2',DWConvLayer(out_channels, out_channels, stride=stride))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return super().forward(x)\n",
    "class HarDBlock(nn.Module):\n",
    "    def get_link(self, layer, base_ch, growth_rate, grmul):\n",
    "        if layer == 0:\n",
    "          return base_ch, 0, []\n",
    "        out_channels = growth_rate\n",
    "        link = []\n",
    "        for i in range(10):\n",
    "          dv = 2 ** i\n",
    "          if layer % dv == 0:\n",
    "            k = layer - dv\n",
    "            link.append(k)\n",
    "            if i > 0:\n",
    "                out_channels *= grmul\n",
    "        out_channels = int(int(out_channels + 1) / 2) * 2\n",
    "        in_channels = 0\n",
    "        for i in link:\n",
    "          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n",
    "          in_channels += ch\n",
    "        return out_channels, in_channels, link\n",
    "\n",
    "    def get_out_ch(self):\n",
    "        return self.out_channels\n",
    "\n",
    "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False):\n",
    "        super().__init__()\n",
    "        self.keepBase = keepBase\n",
    "        self.links = []\n",
    "        layers_ = []\n",
    "        self.out_channels = 0 # if upsample else in_channels\n",
    "        for i in range(n_layers):\n",
    "          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n",
    "          self.links.append(link)\n",
    "          use_relu = residual_out\n",
    "          if dwconv:\n",
    "            layers_.append(CombConvLayer(inch, outch))\n",
    "          else:\n",
    "            layers_.append(ConvLayer(inch, outch))\n",
    "          \n",
    "          if (i % 2 == 0) or (i == n_layers - 1):\n",
    "            self.out_channels += outch\n",
    "        #print(\"Blk out =\",self.out_channels)\n",
    "        self.layers = nn.ModuleList(layers_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        layers_ = [x]\n",
    "        \n",
    "        for layer in range(len(self.layers)):\n",
    "            link = self.links[layer]\n",
    "            tin = []\n",
    "            for i in link:\n",
    "                tin.append(layers_[i])\n",
    "            if len(tin) > 1:            \n",
    "                x = torch.cat(tin, 1)\n",
    "            else:\n",
    "                x = tin[0]\n",
    "            out = self.layers[layer](x)\n",
    "            layers_.append(out)\n",
    "            \n",
    "        t = len(layers_)\n",
    "        out_ = []\n",
    "        for i in range(t):\n",
    "          if (i == 0 and self.keepBase) or \\\n",
    "             (i == t-1) or (i%2 == 1):\n",
    "              out_.append(layers_[i])\n",
    "        out = torch.cat(out_, 1)\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, gr, grmul, n_layer, out_channels, depth_wise):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.hardblock = HarDBlock(in_channels, gr, grmul, n_layer, dwconv = depth_wise)\n",
    "        conv_in_ch = self.hardblock.get_out_ch()\n",
    "        self.conv = ConvLayer(conv_in_ch, out_channels, kernel=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hardblock(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# class HarDNetBackbone(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         in_channels=1,\n",
    "#         base_out_ch=[32, 64],\n",
    "#         grmul=1.7,\n",
    "#         drop_rate=0.1,\n",
    "#         ch_list=[128, 256, 320, 640, 1024],\n",
    "#         gr_list=[14, 16, 20, 40, 160],\n",
    "#         n_layers=[8, 16, 16, 16, 4],\n",
    "#         pool_layer=[1, 0, 1, 1, 0]\n",
    "#     ):\n",
    "#         super(HarDNetBackbone, self).__init__()\n",
    "\n",
    "#         assert len(ch_list) == len(gr_list) == len(n_layers), \"Length of ch_list, gr_list, and n_layers must match\"\n",
    "\n",
    "#         self.base_conv_1 = ConvLayer(in_channels=in_channels, out_channels=base_out_ch[0], kernel=3, stride=2, bias=False)\n",
    "#         self.base_conv_2 = ConvLayer(in_channels=base_out_ch[0], out_channels=base_out_ch[1], kernel=3)\n",
    "#         self.base_max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         self.encoder_blocks = nn.ModuleList()\n",
    "#         self.encoder_pools = nn.ModuleList()\n",
    "#         self.attention_channels = [base_out_ch[1]]  # First attention from base_conv2\n",
    "#         self.pool_layer = pool_layer\n",
    "#         in_ch = base_out_ch[1]\n",
    "#         for i in range(len(ch_list)):\n",
    "#             block = EncoderBlock(in_ch, gr_list[i], grmul, n_layers[i], ch_list[i])\n",
    "#             self.encoder_blocks.append(block)\n",
    "#             self.attention_channels.append(ch_list[i])\n",
    "#             self.encoder_pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "#             in_ch = ch_list[i]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         attention_list = []\n",
    "#         x = self.base_conv_1(x)\n",
    "#         x = self.base_conv_2(x)\n",
    "#         attention_list.append(x)\n",
    "        \n",
    "#         x = self.base_max_pool(x)\n",
    "\n",
    "#         for i, block in enumerate(self.encoder_blocks):\n",
    "#             x = block(x)\n",
    "#             attention_list.append(x)\n",
    "#             if self.pool_layer[i]:\n",
    "#                 x = self.encoder_pools[i](x)\n",
    "\n",
    "#         return attention_list\n",
    "class DWConvLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels,  stride=1,  bias=False):\n",
    "        super().__init__()\n",
    "        out_ch = out_channels\n",
    "        \n",
    "        groups = in_channels\n",
    "        kernel = 3\n",
    "        #print(kernel, 'x', kernel, 'x', out_channels, 'x', out_channels, 'DepthWise')\n",
    "        \n",
    "        self.add_module('dwconv', nn.Conv2d(groups, groups, kernel_size=3,\n",
    "                                          stride=stride, padding=1, groups=groups, bias=bias))\n",
    "        self.add_module('norm', nn.BatchNorm2d(groups))\n",
    "    def forward(self, x):\n",
    "        return super().forward(x)  \n",
    "class HarDNetBackbone(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        base_out_ch=[32, 64],\n",
    "        grmul=1.7,\n",
    "        drop_rate=0.1,\n",
    "        ch_list=[128, 256, 320, 640, 1024],\n",
    "        gr_list=[14, 16, 20, 40, 160],\n",
    "        n_layers=[8, 16, 16, 16, 4],\n",
    "        pool_layer=[1, 0, 1, 1, 0],\n",
    "        depth_wise = False\n",
    "    ):\n",
    "        super(HarDNetBackbone, self).__init__()\n",
    "\n",
    "        assert len(ch_list) == len(gr_list) == len(n_layers), \"Length of ch_list, gr_list, and n_layers must match\"\n",
    "\n",
    "        self.base_conv_1 = ConvLayer(in_channels=in_channels, out_channels=base_out_ch[0], kernel=3, stride=2, bias=False)\n",
    "        if depth_wise == False:\n",
    "            self.base_conv_2 = ConvLayer(in_channels=base_out_ch[0], out_channels=base_out_ch[1], kernel=3)\n",
    "        else:\n",
    "            self.base_conv_2 = ConvLayer(in_channels=base_out_ch[0], out_channels=base_out_ch[1], kernel=1)\n",
    "        # 加入 dropout\n",
    "        self.base_dropout = nn.Dropout2d(p=drop_rate)\n",
    "        if depth_wise == False:\n",
    "            self.base_down = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.base_down = DWConvLayer(base_out_ch[1], base_out_ch[1], stride=2)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.encoder_down = nn.ModuleList()\n",
    "        self.encoder_dropouts = nn.ModuleList()  # 加一個 dropout list\n",
    "        self.attention_channels = [base_out_ch[1]]\n",
    "        self.pool_layer = pool_layer\n",
    "\n",
    "        in_ch = base_out_ch[1]\n",
    "        for i in range(len(ch_list)):\n",
    "            block = EncoderBlock(in_ch, gr_list[i], grmul, n_layers[i], ch_list[i], depth_wise = depth_wise)\n",
    "            self.encoder_blocks.append(block)\n",
    "            self.attention_channels.append(ch_list[i])\n",
    "            if depth_wise == False:\n",
    "                self.encoder_down.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                self.encoder_down.append(DWConvLayer(ch_list[i], ch_list[i], stride=2))\n",
    "            self.encoder_dropouts.append(nn.Dropout2d(p=drop_rate))  # 對應 encoder block 加入 Dropout\n",
    "            in_ch = ch_list[i]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "            attention_list = []\n",
    "            x = self.base_conv_1(x)\n",
    "            x = self.base_conv_2(x)\n",
    "            x = self.base_dropout(x)\n",
    "        \n",
    "            attention_list.append(x)\n",
    "            \n",
    "            x = self.base_down(x)\n",
    "    \n",
    "            for i, block in enumerate(self.encoder_blocks):\n",
    "                x = block(x)\n",
    "                x = self.encoder_dropouts[i](x)\n",
    "                attention_list.append(x)\n",
    "                if self.pool_layer[i]:\n",
    "                    x = self.encoder_down[i](x)\n",
    "    \n",
    "            return attention_list\n",
    "\n",
    "class Synthesize_AdaptiveMaxPool(nn.Module):\n",
    "    def __init__(self, target_size):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        kernel_size = (2,2)\n",
    "        x = F.interpolate(x, size=(self.target_size[0] * kernel_size[0], self.target_size[1] * kernel_size[1]), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return F.max_pool2d(x, kernel_size=kernel_size, stride=(1, 1))\n",
    "class AMFF(nn.Module):\n",
    "    def __init__(self, n_inputs=5, in_channels = [0, 0, 0, 0, 0], out_channels = [12, 12, 12, 12, 12], synthesize_adaptive_max_pool = False):\n",
    "        super(AMFF, self).__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        max_pool_size = (20, 20)\n",
    "        if synthesize_adaptive_max_pool:\n",
    "            self.max_pools = nn.ModuleList([\n",
    "                Synthesize_AdaptiveMaxPool(max_pool_size) for _ in range(n_inputs)\n",
    "            ])\n",
    "        else:\n",
    "            self.max_pools = nn.ModuleList([\n",
    "                nn.AdaptiveMaxPool2d(max_pool_size) for _ in range(n_inputs)\n",
    "            ])\n",
    "\n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels[i], out_channels[i], kernel_size = 3, padding=1) for i in range(n_inputs)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        x_outs = []\n",
    "        for i in range(self.n_inputs):\n",
    "            x_out = self.max_pools[i](x_list[i])\n",
    "            x_out = self.conv[i](x_out)\n",
    "            x_outs.append(x_out)\n",
    "        x_cat = torch.cat(x_outs, dim=1)\n",
    "        \n",
    "        return x_cat\n",
    "\n",
    "class PMCS(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(PMCS, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        HW = H * W\n",
    "\n",
    "        # Q: [B, C, H, W] -> [B, C, HW]\n",
    "        Q = self.query_conv(x).reshape(B, C, HW)\n",
    "\n",
    "        # K: [B, 1, H, W] -> [B, 1, HW] and softmax\n",
    "        K = self.key_conv(x).reshape(B, 1, HW)\n",
    "        K = F.softmax(K, dim=-1)\n",
    "\n",
    "        # MatMul(Q, K^T): [B, C, HW] @ [B, HW, 1] -> [B, C, 1]\n",
    "        attn = torch.bmm(Q, K.transpose(1, 2)).view(B, C, 1, 1)\n",
    "\n",
    "        # Conv + LayerNorm + Sigmoid\n",
    "        attn = self.out_conv(attn)  # [B, C, 1, 1]\n",
    "        attn = self.norm(attn.squeeze(-1).squeeze(-1)).unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]\n",
    "        attn = self.sigmoid(attn)\n",
    "\n",
    "        # V: [B, C, H, W]\n",
    "        V = self.value_conv(x)\n",
    "\n",
    "        # Final Output: V * attn\n",
    "        out = V * attn  # broadcasting over (H, W)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PMSS(nn.Module):\n",
    "    def __init__(self, in_channels, n_branches=3):\n",
    "        super(PMSS, self).__init__()\n",
    "        self.n_branches = n_branches\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels * n_branches, kernel_size=1)\n",
    "        self.output_conv = nn.Conv2d(in_channels * n_branches, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Assume x is already multi-scale fused: shape (B, 3Ck, H, W)\n",
    "        Ck = C // self.n_branches  # Channel per branch\n",
    "        Cv = Ck  # Value channels per branch (can be different if designed that way)\n",
    "\n",
    "        # Step 1: Compute Q and K\n",
    "        Q = self.query_conv(x)  # (B, C, H, W)\n",
    "        K = self.key_conv(x)    # (B, C, H, W)\n",
    "\n",
    "        # Step 2: Global mean pooling across spatial dimensions on K\n",
    "        K_pool = F.adaptive_avg_pool2d(K, output_size=1)  # (B, C, 1, 1)\n",
    "        K_pool = K_pool.view(B, C)                        # (B, C)\n",
    "        K_soft = F.softmax(K_pool, dim=1)                 # (B, C)\n",
    "\n",
    "        # Step 3: Reshape Q and K to (B, C, HW) and perform matmul\n",
    "        Q_flat = Q.view(B, C, -1)                         # (B, C, HW)\n",
    "        K_soft = K_soft.view(B, C, 1)                     # (B, C, 1)\n",
    "        attention_scores = torch.bmm(K_soft.transpose(1, 2), Q_flat)  # (B, 1, HW)\n",
    "        attention_scores = attention_scores.view(B, 1, H, W)          # (B, 1, H, W)\n",
    "        attention_map = self.sigmoid(attention_scores)                # (B, 1, H, W)\n",
    "\n",
    "        # Step 4: Value computation\n",
    "        V = self.value_conv(x)                            # (B, Cv, H, W)\n",
    "        V_split = torch.chunk(V, self.n_branches, dim=1)  # [(B, Cv, H, W)] * 3\n",
    "\n",
    "        # Repeat attention for each branch and multiply\n",
    "        attended = [v * attention_map for v in V_split]   # [(B, Cv, H, W)] * 3\n",
    "\n",
    "        # Step 5: Concatenate and project\n",
    "        fused = torch.cat(attended, dim=1)                # (B, Cv * 3, H, W)\n",
    "        out = self.output_conv(fused)                     # (B, Cv, H, W)\n",
    "\n",
    "        return out\n",
    "\n",
    "class PMFS(nn.Module):\n",
    "    def __init__(self, n_inputs, in_channels, synthesize_adaptive_max_pool = False):\n",
    "        super(PMFS, self).__init__()\n",
    "        self.amff_out_channels = [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12][:n_inputs]\n",
    "        self.amff = AMFF(n_inputs = n_inputs, in_channels = in_channels, out_channels = self.amff_out_channels, synthesize_adaptive_max_pool = synthesize_adaptive_max_pool)\n",
    "        \n",
    "        self.attention_in_channels = sum(self.amff_out_channels)\n",
    "        self.pmcs = PMCS(in_channels = self.attention_in_channels)\n",
    "        self.pmss = PMSS(in_channels = self.attention_in_channels, n_branches = len(self.amff_out_channels))\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        amff_out = self.amff(x_list)\n",
    "        pmcs_out = self.pmcs(amff_out)\n",
    "        # print(pmcs_out.shape)\n",
    "        pmss_out = self.pmss(pmcs_out)\n",
    "        return pmss_out\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=60, out_channels=1, output_size=224, layers_num=4):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        assert 1 <= layers_num <= 4, \"layers_num 必須是 1 到 4 之間\"\n",
    "\n",
    "        self.layers_num = layers_num\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # 全部 4 層的設計：對應 (in → out, size)\n",
    "        channels = [64, 32, 16, 8][-layers_num:]\n",
    "        channels = [in_channels] + channels\n",
    "        # print(channels)\n",
    "        self.all_ups = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[i], channels[i + 1], kernel_size=2, stride=2) for i in range(layers_num)\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.all_convs = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv2d(channels[i + 1], channels[i + 1], kernel_size=3, padding=1), nn.ReLU(inplace=True)) for i in range(layers_num)\n",
    "        ])\n",
    "\n",
    "        # 根據 layers_num 只保留最後 N 層\n",
    "        self.ups = self.all_ups\n",
    "        self.convs = self.all_convs\n",
    "\n",
    "        # 輸出層根據最後一層 conv 的 output channel 決定\n",
    "        out_ch = 8\n",
    "        self.out_conv = nn.Conv2d(out_ch, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for up, conv in zip(self.ups, self.convs):\n",
    "            # print(\"origin\")\n",
    "            # print(x.shape)\n",
    "            x = up(x)\n",
    "            # print(x.shape)\n",
    "            x = conv(x)\n",
    "            # print(x.shape)\n",
    "\n",
    "        x = F.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(x)\n",
    "        \n",
    "class UpsampleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=32, output_size=224):\n",
    "        super(UpsampleConvBlock, self).__init__()\n",
    "        self.conv = ConvLayer(in_channels, out_channels, kernel=3)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "        \n",
    "class HybridSegModel(nn.Module):\n",
    "    def __init__(self, in_channels = 1, out_channels = 2, output_size = 224, layers_num = 5, dropout_rate = 0.0, arch = None, depth_wise = False, synthesize_adaptive_max_pool = False):\n",
    "        super(HybridSegModel, self).__init__()\n",
    "\n",
    "        if arch == 68:\n",
    "            base_out_ch  = [32, 64]\n",
    "            grmul = 1.7\n",
    "            ch_list=[128, 256, 320, 640, 1024]\n",
    "            gr_list=[14, 16, 20, 40, 160]\n",
    "            n_layers=[8, 16, 16, 16, 4]\n",
    "            pool_layer=[1, 0, 1, 1, 0]\n",
    "        elif arch == 39:\n",
    "            base_out_ch  = [24, 48]\n",
    "            ch_list = [  96, 320, 640, 1024]\n",
    "            grmul = 1.6\n",
    "            gr_list = [  16,  20, 64, 160]\n",
    "            n_layers = [   4,  16,  8,   4]\n",
    "            pool_layer = [   1,   1,  1,   0]\n",
    "\n",
    "        self.layers_num = layers_num\n",
    "        self.backbone = HarDNetBackbone(in_channels, base_out_ch = base_out_ch, grmul = grmul, ch_list = ch_list[:layers_num], gr_list = gr_list[:layers_num], n_layers = n_layers[:layers_num], pool_layer = pool_layer[:layers_num], drop_rate = dropout_rate, depth_wise = depth_wise)\n",
    "\n",
    "        n_attention = layers_num + 1\n",
    "        pmfs_in_channels = self.backbone.attention_channels\n",
    "        self.pmfs = PMFS(n_inputs = n_attention, in_channels = pmfs_in_channels, synthesize_adaptive_max_pool = synthesize_adaptive_max_pool)\n",
    "\n",
    "        decoder_in_channels = self.pmfs.attention_in_channels\n",
    "        decoder_out_channels = 32\n",
    "        decoder_layers = layers_num - 1\n",
    "        self.decoder = Decoder(in_channels = decoder_in_channels, out_channels = decoder_out_channels, output_size = output_size, layers_num = decoder_layers)\n",
    "\n",
    "        \n",
    "        self.upsample_list = nn.ModuleList([\n",
    "            UpsampleConvBlock(base_out_ch[1], out_channels=32, output_size=output_size),\n",
    "        ])\n",
    "        for i in range(len(ch_list)):\n",
    "            self.upsample_list.append(UpsampleConvBlock(ch_list[i], out_channels=32, output_size=output_size))\n",
    "\n",
    "        final_in_channels = self.layers_num * 32 + decoder_out_channels\n",
    "        self.final_conv = nn.Conv2d(final_in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        attention_list = self.backbone(x)\n",
    "        attention_upsample = []\n",
    "        for i in range(self.layers_num):\n",
    "            upsample = self.upsample_list[i](attention_list[i])\n",
    "            attention_upsample.append(upsample)\n",
    "        pmfs_out = self.pmfs(attention_list)\n",
    "        out = self.decoder(pmfs_out)\n",
    "\n",
    "        attention_upsample_cat = torch.cat(attention_upsample, axis = 1)\n",
    "        out_cat = torch.cat([attention_upsample_cat, out], axis = 1)\n",
    "\n",
    "        out = self.final_conv(out_cat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c80a24-8a40-4281-a53e-a07b43edb569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f5baf66-d5c5-461d-872e-9693d15fa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sime = 128\n",
    "input_example = torch.zeros((8, 1, image_sime, image_sime)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88b2ad46-ad02-4b48-be92-0d52689dc889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "model = HybridSegModel(in_channels = 1, out_channels = 2, output_size = image_size, layers_num = 3, dropout_rate = 0.0, arch = 68, depth_wise = True, synthesize_adaptive_max_pool = True).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a461bfd-74b9-4b51-863a-dac96c7a39cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_example = model(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dc3a5b3-83a6-4be9-bf38-c22dfc412fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,318,245\n",
      "Trainable parameters: 1,318,245\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46c65d4c-fea9-4b15-b489-931380c6b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "model = HybridSegModel(in_channels = 1, out_channels = 2, output_size = image_size, layers_num = 3, dropout_rate = 0.0, arch = 39, depth_wise = True).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "004be7bb-32ad-4942-8a94-4e3a1e696f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 48, 32, 32])\n",
      "torch.Size([8, 96, 16, 16])\n",
      "torch.Size([8, 320, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "output_example = model(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5282b0a9-a0e4-4435-915a-adb3a99d84b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,689,389\n",
      "Trainable parameters: 1,689,389\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcd894-302a-4bc4-bb36-e9c1325d2cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thyroid",
   "language": "python",
   "name": "thyroid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
