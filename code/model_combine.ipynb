{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69c56085-c604-4801-8f4f-10095e947231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1, bias=False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        groups = 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=kernel//2, groups=groups, bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU6(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class HarDBlock(nn.Module):\n",
    "    def get_link(self, layer, base_ch, growth_rate, grmul):\n",
    "        if layer == 0:\n",
    "          return base_ch, 0, []\n",
    "        out_channels = growth_rate\n",
    "        link = []\n",
    "        for i in range(10):\n",
    "          dv = 2 ** i\n",
    "          if layer % dv == 0:\n",
    "            k = layer - dv\n",
    "            link.append(k)\n",
    "            if i > 0:\n",
    "                out_channels *= grmul\n",
    "        out_channels = int(int(out_channels + 1) / 2) * 2\n",
    "        in_channels = 0\n",
    "        for i in link:\n",
    "          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n",
    "          in_channels += ch\n",
    "        return out_channels, in_channels, link\n",
    "\n",
    "    def get_out_ch(self):\n",
    "        return self.out_channels\n",
    "\n",
    "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False):\n",
    "        super().__init__()\n",
    "        self.keepBase = keepBase\n",
    "        self.links = []\n",
    "        layers_ = []\n",
    "        self.out_channels = 0 # if upsample else in_channels\n",
    "        for i in range(n_layers):\n",
    "          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n",
    "          self.links.append(link)\n",
    "          use_relu = residual_out\n",
    "          if dwconv:\n",
    "            layers_.append(CombConvLayer(inch, outch))\n",
    "          else:\n",
    "            layers_.append(ConvLayer(inch, outch))\n",
    "          \n",
    "          if (i % 2 == 0) or (i == n_layers - 1):\n",
    "            self.out_channels += outch\n",
    "        #print(\"Blk out =\",self.out_channels)\n",
    "        self.layers = nn.ModuleList(layers_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        layers_ = [x]\n",
    "        \n",
    "        for layer in range(len(self.layers)):\n",
    "            link = self.links[layer]\n",
    "            tin = []\n",
    "            for i in link:\n",
    "                tin.append(layers_[i])\n",
    "            if len(tin) > 1:            \n",
    "                x = torch.cat(tin, 1)\n",
    "            else:\n",
    "                x = tin[0]\n",
    "            out = self.layers[layer](x)\n",
    "            layers_.append(out)\n",
    "            \n",
    "        t = len(layers_)\n",
    "        out_ = []\n",
    "        for i in range(t):\n",
    "          if (i == 0 and self.keepBase) or \\\n",
    "             (i == t-1) or (i%2 == 1):\n",
    "              out_.append(layers_[i])\n",
    "        out = torch.cat(out_, 1)\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, gr, grmul, n_layer, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.hardblock = HarDBlock(in_channels, gr, grmul, n_layer)\n",
    "        conv_in_ch = self.hardblock.get_out_ch()\n",
    "        self.conv = ConvLayer(conv_in_ch, out_channels, kernel=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hardblock(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class HarDNetBackbone(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        base_out_ch=[32, 64],\n",
    "        grmul=1.7,\n",
    "        drop_rate=0.1,\n",
    "        ch_list=[128, 256, 320, 640, 1024],\n",
    "        gr_list=[14, 16, 20, 40, 160],\n",
    "        n_layers=[8, 16, 16, 16, 4],\n",
    "        pool_layer=[1, 0, 1, 1, 0]\n",
    "    ):\n",
    "        super(HarDNetBackbone, self).__init__()\n",
    "\n",
    "        assert len(ch_list) == len(gr_list) == len(n_layers), \"Length of ch_list, gr_list, and n_layers must match\"\n",
    "\n",
    "        self.base_conv_1 = ConvLayer(in_channels=in_channels, out_channels=base_out_ch[0], kernel=3, stride=2, bias=False)\n",
    "        self.base_conv_2 = ConvLayer(in_channels=base_out_ch[0], out_channels=base_out_ch[1], kernel=3)\n",
    "        self.base_max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.encoder_pools = nn.ModuleList()\n",
    "        self.attention_channels = [base_out_ch[1]]  # First attention from base_conv2\n",
    "        self.pool_layer = pool_layer\n",
    "        in_ch = base_out_ch[1]\n",
    "        for i in range(len(ch_list)):\n",
    "            block = EncoderBlock(in_ch, gr_list[i], grmul, n_layers[i], ch_list[i])\n",
    "            self.encoder_blocks.append(block)\n",
    "            self.attention_channels.append(ch_list[i])\n",
    "            self.encoder_pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_ch = ch_list[i]\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_list = []\n",
    "        x = self.base_conv_1(x)\n",
    "        x = self.base_conv_2(x)\n",
    "        attention_list.append(x)\n",
    "        \n",
    "        x = self.base_max_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.encoder_blocks):\n",
    "            x = block(x)\n",
    "            attention_list.append(x)\n",
    "            if self.pool_layer[i]:\n",
    "                x = self.encoder_pools[i](x)\n",
    "\n",
    "        return attention_list\n",
    "class AMFF(nn.Module):\n",
    "    def __init__(self, n_inputs=5, in_channels = [0, 0, 0, 0, 0], out_channels = [12, 12, 12, 12, 12]):\n",
    "        super(AMFF, self).__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        max_pool_size = (20, 20)\n",
    "        self.max_pools = nn.ModuleList([\n",
    "            nn.AdaptiveMaxPool2d(max_pool_size) for _ in range(n_inputs)\n",
    "        ])\n",
    "\n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels[i], out_channels[i], kernel_size = 3, padding=1) for i in range(n_inputs)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        x_outs = []\n",
    "        for i in range(self.n_inputs):\n",
    "            x_out = self.max_pools[i](x_list[i])\n",
    "            x_out = self.conv[i](x_out)\n",
    "            x_outs.append(x_out)\n",
    "        x_cat = torch.cat(x_outs, dim=1)\n",
    "        \n",
    "        return x_cat\n",
    "\n",
    "class PMCS(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(PMCS, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        HW = H * W\n",
    "\n",
    "        # Q: [B, C, H, W] -> [B, C, HW]\n",
    "        Q = self.query_conv(x).reshape(B, C, HW)\n",
    "\n",
    "        # K: [B, 1, H, W] -> [B, 1, HW] and softmax\n",
    "        K = self.key_conv(x).reshape(B, 1, HW)\n",
    "        K = F.softmax(K, dim=-1)\n",
    "\n",
    "        # MatMul(Q, K^T): [B, C, HW] @ [B, HW, 1] -> [B, C, 1]\n",
    "        attn = torch.bmm(Q, K.transpose(1, 2)).view(B, C, 1, 1)\n",
    "\n",
    "        # Conv + LayerNorm + Sigmoid\n",
    "        attn = self.out_conv(attn)  # [B, C, 1, 1]\n",
    "        attn = self.norm(attn.squeeze(-1).squeeze(-1)).unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]\n",
    "        attn = self.sigmoid(attn)\n",
    "\n",
    "        # V: [B, C, H, W]\n",
    "        V = self.value_conv(x)\n",
    "\n",
    "        # Final Output: V * attn\n",
    "        out = V * attn  # broadcasting over (H, W)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PMSS(nn.Module):\n",
    "    def __init__(self, in_channels, n_branches=3):\n",
    "        super(PMSS, self).__init__()\n",
    "        self.n_branches = n_branches\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels * n_branches, kernel_size=1)\n",
    "        self.output_conv = nn.Conv2d(in_channels * n_branches, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Assume x is already multi-scale fused: shape (B, 3Ck, H, W)\n",
    "        Ck = C // self.n_branches  # Channel per branch\n",
    "        Cv = Ck  # Value channels per branch (can be different if designed that way)\n",
    "\n",
    "        # Step 1: Compute Q and K\n",
    "        Q = self.query_conv(x)  # (B, C, H, W)\n",
    "        K = self.key_conv(x)    # (B, C, H, W)\n",
    "\n",
    "        # Step 2: Global mean pooling across spatial dimensions on K\n",
    "        K_pool = F.adaptive_avg_pool2d(K, output_size=1)  # (B, C, 1, 1)\n",
    "        K_pool = K_pool.view(B, C)                        # (B, C)\n",
    "        K_soft = F.softmax(K_pool, dim=1)                 # (B, C)\n",
    "\n",
    "        # Step 3: Reshape Q and K to (B, C, HW) and perform matmul\n",
    "        Q_flat = Q.view(B, C, -1)                         # (B, C, HW)\n",
    "        K_soft = K_soft.view(B, C, 1)                     # (B, C, 1)\n",
    "        attention_scores = torch.bmm(K_soft.transpose(1, 2), Q_flat)  # (B, 1, HW)\n",
    "        attention_scores = attention_scores.view(B, 1, H, W)          # (B, 1, H, W)\n",
    "        attention_map = self.sigmoid(attention_scores)                # (B, 1, H, W)\n",
    "\n",
    "        # Step 4: Value computation\n",
    "        V = self.value_conv(x)                            # (B, Cv, H, W)\n",
    "        V_split = torch.chunk(V, self.n_branches, dim=1)  # [(B, Cv, H, W)] * 3\n",
    "\n",
    "        # Repeat attention for each branch and multiply\n",
    "        attended = [v * attention_map for v in V_split]   # [(B, Cv, H, W)] * 3\n",
    "\n",
    "        # Step 5: Concatenate and project\n",
    "        fused = torch.cat(attended, dim=1)                # (B, Cv * 3, H, W)\n",
    "        out = self.output_conv(fused)                     # (B, Cv, H, W)\n",
    "\n",
    "        return out\n",
    "\n",
    "class PMFS(nn.Module):\n",
    "    def __init__(self, n_inputs, in_channels):\n",
    "        super(PMFS, self).__init__()\n",
    "        self.amff_out_channels = [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12][:n_inputs]\n",
    "        self.amff = AMFF(n_inputs = n_inputs, in_channels = in_channels, out_channels = self.amff_out_channels)\n",
    "        \n",
    "        self.attention_in_channels = sum(self.amff_out_channels)\n",
    "        self.pmcs = PMCS(in_channels = self.attention_in_channels)\n",
    "        self.pmss = PMSS(in_channels = self.attention_in_channels, n_branches = len(self.amff_out_channels))\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        amff_out = self.amff(x_list)\n",
    "        pmcs_out = self.pmcs(amff_out)\n",
    "        # print(pmcs_out.shape)\n",
    "        pmss_out = self.pmss(pmcs_out)\n",
    "        return pmss_out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=60, out_channels=1, output_size = 224):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(in_channels, 64, kernel_size=2, stride=2)  # (20 → 40)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)  # (40 → 80)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)  # (80 → 160)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)   # (160 → 320)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv2d(8, out_channels, kernel_size=1)  # map to [B, 1, 224, 224]\n",
    "\n",
    "        self.output_size = output_size\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)       # [B, 64, 40, 40]\n",
    "        x = self.conv1(x)\n",
    "        x = self.up2(x)       # [B, 32, 80, 80]\n",
    "        x = self.conv2(x)\n",
    "        x = self.up3(x)       # [B, 16, 160, 160]\n",
    "        x = self.conv3(x)\n",
    "        x = self.up4(x)       # [B, 8, 320, 320]\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = F.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "        return torch.sigmoid(self.out_conv(x))  # [B, 1, 224, 224]\n",
    "        \n",
    "class UpsampleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=32, output_size=224):\n",
    "        super(UpsampleConvBlock, self).__init__()\n",
    "        self.conv = ConvLayer(in_channels, out_channels, kernel=3)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "        \n",
    "class HybridSegModel(nn.Module):\n",
    "    def __init__(self, in_channels = 1, out_channels = 2, output_size = 224, layers_num = 5):\n",
    "        super(HybridSegModel, self).__init__()\n",
    "\n",
    "        ch_list=[128, 256, 320, 640, 1024]\n",
    "        gr_list=[14, 16, 20, 40, 160]\n",
    "        n_layers=[8, 16, 16, 16, 4]\n",
    "        pool_layer=[1, 0, 1, 1, 0]\n",
    "        \n",
    "        self.layers_num = layers_num\n",
    "        self.backbone = HarDNetBackbone(in_channels, ch_list = ch_list[:layers_num], gr_list = gr_list[:layers_num], n_layers = n_layers[:layers_num], pool_layer = pool_layer[:layers_num])\n",
    "\n",
    "        n_attention = layers_num + 1\n",
    "        pmfs_in_channels = self.backbone.attention_channels\n",
    "        self.pmfs = PMFS(n_inputs = n_attention, in_channels = pmfs_in_channels)\n",
    "\n",
    "        decoder_in_channels = self.pmfs.attention_in_channels\n",
    "        decoder_out_channels = 32\n",
    "        self.decoder = Decoder(in_channels = decoder_in_channels, out_channels = decoder_out_channels, output_size = output_size)\n",
    "        \n",
    "        self.upsample_list = nn.ModuleList([\n",
    "            UpsampleConvBlock(64, out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[0], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[1], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[2], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[3], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[4], out_channels=32, output_size=output_size)\n",
    "        ])\n",
    "\n",
    "        final_in_channels = self.layers_num * 32 + decoder_out_channels\n",
    "        self.final_conv = nn.Conv2d(final_in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        attention_list = self.backbone(x)\n",
    "        attention_upsample = []\n",
    "        for i in range(self.layers_num):\n",
    "            upsample = self.upsample_list[i](attention_list[i])\n",
    "            attention_upsample.append(upsample)\n",
    "            # print(upsample.shape)\n",
    "        pmfs_out = self.pmfs(attention_list)\n",
    "        out = self.decoder(pmfs_out)\n",
    "\n",
    "        attention_upsample_cat = torch.cat(attention_upsample, axis = 1)\n",
    "        out_cat = torch.cat([attention_upsample_cat, out], axis = 1)\n",
    "\n",
    "        out = self.final_conv(out_cat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "776c9755-b8f5-4802-9b19-e8eb67895bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridSegModel(in_channels = 1, out_channels = 2, layers_num = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9067094f-011a-474a-b2ff-35299297ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b34e2dd1-4647-4af8-b3a6-2872d73b331e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa12497a-28c9-49f3-bd83-aea14a20b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    print(pred[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9fa0900-696c-4b1d-81e8-d52036d432d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3,599,815\n",
      "Trainable parameters: 3,599,815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from HarDMSEG import HarDMSEG\n",
    "\n",
    "model = HybridSegModel(in_channels = 3, out_channels = 2, layers_num = 3)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "439df8ad-1a71-446c-9c80-f59c0bf1981a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average FPS over 200 images: 13.573466385517143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import torch\n",
    "total_time = 0\n",
    "frame_num = 200\n",
    "for i in range(frame_num):\n",
    "    input_image = torch.zeros((1, 3, 256, 256))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output = model(input_image)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time += (end_time - start_time)\n",
    "avg_time = total_time / frame_num\n",
    "fps = 1 / avg_time\n",
    "print(f\"Average FPS over {frame_num} images: {fps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77380e-fe28-41f0-a497-7b7efc2477cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thyroid",
   "language": "python",
   "name": "thyroid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
