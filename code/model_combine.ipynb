{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "69c56085-c604-4801-8f4f-10095e947231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1, bias=False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        groups = 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel, stride=stride, padding=kernel//2, groups=groups, bias=bias)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU6(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class HarDBlock(nn.Module):\n",
    "    def get_link(self, layer, base_ch, growth_rate, grmul):\n",
    "        if layer == 0:\n",
    "          return base_ch, 0, []\n",
    "        out_channels = growth_rate\n",
    "        link = []\n",
    "        for i in range(10):\n",
    "          dv = 2 ** i\n",
    "          if layer % dv == 0:\n",
    "            k = layer - dv\n",
    "            link.append(k)\n",
    "            if i > 0:\n",
    "                out_channels *= grmul\n",
    "        out_channels = int(int(out_channels + 1) / 2) * 2\n",
    "        in_channels = 0\n",
    "        for i in link:\n",
    "          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n",
    "          in_channels += ch\n",
    "        return out_channels, in_channels, link\n",
    "\n",
    "    def get_out_ch(self):\n",
    "        return self.out_channels\n",
    "\n",
    "    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False, dwconv=False):\n",
    "        super().__init__()\n",
    "        self.keepBase = keepBase\n",
    "        self.links = []\n",
    "        layers_ = []\n",
    "        self.out_channels = 0 # if upsample else in_channels\n",
    "        for i in range(n_layers):\n",
    "          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n",
    "          self.links.append(link)\n",
    "          use_relu = residual_out\n",
    "          if dwconv:\n",
    "            layers_.append(CombConvLayer(inch, outch))\n",
    "          else:\n",
    "            layers_.append(ConvLayer(inch, outch))\n",
    "          \n",
    "          if (i % 2 == 0) or (i == n_layers - 1):\n",
    "            self.out_channels += outch\n",
    "        #print(\"Blk out =\",self.out_channels)\n",
    "        self.layers = nn.ModuleList(layers_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        layers_ = [x]\n",
    "        \n",
    "        for layer in range(len(self.layers)):\n",
    "            link = self.links[layer]\n",
    "            tin = []\n",
    "            for i in link:\n",
    "                tin.append(layers_[i])\n",
    "            if len(tin) > 1:            \n",
    "                x = torch.cat(tin, 1)\n",
    "            else:\n",
    "                x = tin[0]\n",
    "            out = self.layers[layer](x)\n",
    "            layers_.append(out)\n",
    "            \n",
    "        t = len(layers_)\n",
    "        out_ = []\n",
    "        for i in range(t):\n",
    "          if (i == 0 and self.keepBase) or \\\n",
    "             (i == t-1) or (i%2 == 1):\n",
    "              out_.append(layers_[i])\n",
    "        out = torch.cat(out_, 1)\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, gr, grmul, n_layer, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.hardblock = HarDBlock(in_channels, gr, grmul, n_layer)\n",
    "        conv_in_ch = self.hardblock.get_out_ch()\n",
    "        self.conv = ConvLayer(conv_in_ch, out_channels, kernel=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hardblock(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class HarDNetBackbone(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        base_out_ch=[32, 64],\n",
    "        grmul=1.7,\n",
    "        drop_rate=0.1,\n",
    "        ch_list=[128, 256, 320, 640, 1024],\n",
    "        gr_list=[14, 16, 20, 40, 160],\n",
    "        n_layers=[8, 16, 16, 16, 4],\n",
    "        pool_layer=[1, 0, 1, 1, 0]\n",
    "    ):\n",
    "        super(HarDNetBackbone, self).__init__()\n",
    "\n",
    "        assert len(ch_list) == len(gr_list) == len(n_layers), \"Length of ch_list, gr_list, and n_layers must match\"\n",
    "\n",
    "        self.base_conv_1 = ConvLayer(in_channels=in_channels, out_channels=base_out_ch[0], kernel=3, stride=2, bias=False)\n",
    "        self.base_conv_2 = ConvLayer(in_channels=base_out_ch[0], out_channels=base_out_ch[1], kernel=3)\n",
    "        self.base_max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.encoder_pools = nn.ModuleList()\n",
    "        self.attention_channels = [base_out_ch[1]]  # First attention from base_conv2\n",
    "        self.pool_layer = pool_layer\n",
    "        in_ch = base_out_ch[1]\n",
    "        for i in range(len(ch_list)):\n",
    "            block = EncoderBlock(in_ch, gr_list[i], grmul, n_layers[i], ch_list[i])\n",
    "            self.encoder_blocks.append(block)\n",
    "            self.attention_channels.append(ch_list[i])\n",
    "            self.encoder_pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_ch = ch_list[i]\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_list = []\n",
    "        x = self.base_conv_1(x)\n",
    "        x = self.base_conv_2(x)\n",
    "        attention_list.append(x)\n",
    "        \n",
    "        x = self.base_max_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.encoder_blocks):\n",
    "            x = block(x)\n",
    "            attention_list.append(x)\n",
    "            if self.pool_layer[i]:\n",
    "                x = self.encoder_pools[i](x)\n",
    "\n",
    "        return attention_list\n",
    "class AMFF(nn.Module):\n",
    "    def __init__(self, n_inputs=5, in_channels = [0, 0, 0, 0, 0], out_channels = [12, 12, 12, 12, 12]):\n",
    "        super(AMFF, self).__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        max_pool_size = (20, 20)\n",
    "        self.max_pools = nn.ModuleList([\n",
    "            nn.AdaptiveMaxPool2d(max_pool_size) for _ in range(n_inputs)\n",
    "        ])\n",
    "\n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels[i], out_channels[i], kernel_size = 3, padding=1) for i in range(n_inputs)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        x_outs = []\n",
    "        for i in range(self.n_inputs):\n",
    "            x_out = self.max_pools[i](x_list[i])\n",
    "            x_out = self.conv[i](x_out)\n",
    "            x_outs.append(x_out)\n",
    "        x_cat = torch.cat(x_outs, dim=1)\n",
    "        \n",
    "        return x_cat\n",
    "\n",
    "class PMCS(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(PMCS, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        HW = H * W\n",
    "\n",
    "        # Q: [B, C, H, W] -> [B, C, HW]\n",
    "        Q = self.query_conv(x).reshape(B, C, HW)\n",
    "\n",
    "        # K: [B, 1, H, W] -> [B, 1, HW] and softmax\n",
    "        K = self.key_conv(x).reshape(B, 1, HW)\n",
    "        K = F.softmax(K, dim=-1)\n",
    "\n",
    "        # MatMul(Q, K^T): [B, C, HW] @ [B, HW, 1] -> [B, C, 1]\n",
    "        attn = torch.bmm(Q, K.transpose(1, 2)).view(B, C, 1, 1)\n",
    "\n",
    "        # Conv + LayerNorm + Sigmoid\n",
    "        attn = self.out_conv(attn)  # [B, C, 1, 1]\n",
    "        attn = self.norm(attn.squeeze(-1).squeeze(-1)).unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]\n",
    "        attn = self.sigmoid(attn)\n",
    "\n",
    "        # V: [B, C, H, W]\n",
    "        V = self.value_conv(x)\n",
    "\n",
    "        # Final Output: V * attn\n",
    "        out = V * attn  # broadcasting over (H, W)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PMSS(nn.Module):\n",
    "    def __init__(self, in_channels, n_branches=3):\n",
    "        super(PMSS, self).__init__()\n",
    "        self.n_branches = n_branches\n",
    "        self.query_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels * n_branches, kernel_size=1)\n",
    "        self.output_conv = nn.Conv2d(in_channels * n_branches, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Assume x is already multi-scale fused: shape (B, 3Ck, H, W)\n",
    "        Ck = C // self.n_branches  # Channel per branch\n",
    "        Cv = Ck  # Value channels per branch (can be different if designed that way)\n",
    "\n",
    "        # Step 1: Compute Q and K\n",
    "        Q = self.query_conv(x)  # (B, C, H, W)\n",
    "        K = self.key_conv(x)    # (B, C, H, W)\n",
    "\n",
    "        # Step 2: Global mean pooling across spatial dimensions on K\n",
    "        K_pool = F.adaptive_avg_pool2d(K, output_size=1)  # (B, C, 1, 1)\n",
    "        K_pool = K_pool.view(B, C)                        # (B, C)\n",
    "        K_soft = F.softmax(K_pool, dim=1)                 # (B, C)\n",
    "\n",
    "        # Step 3: Reshape Q and K to (B, C, HW) and perform matmul\n",
    "        Q_flat = Q.view(B, C, -1)                         # (B, C, HW)\n",
    "        K_soft = K_soft.view(B, C, 1)                     # (B, C, 1)\n",
    "        attention_scores = torch.bmm(K_soft.transpose(1, 2), Q_flat)  # (B, 1, HW)\n",
    "        attention_scores = attention_scores.view(B, 1, H, W)          # (B, 1, H, W)\n",
    "        attention_map = self.sigmoid(attention_scores)                # (B, 1, H, W)\n",
    "\n",
    "        # Step 4: Value computation\n",
    "        V = self.value_conv(x)                            # (B, Cv, H, W)\n",
    "        V_split = torch.chunk(V, self.n_branches, dim=1)  # [(B, Cv, H, W)] * 3\n",
    "\n",
    "        # Repeat attention for each branch and multiply\n",
    "        attended = [v * attention_map for v in V_split]   # [(B, Cv, H, W)] * 3\n",
    "\n",
    "        # Step 5: Concatenate and project\n",
    "        fused = torch.cat(attended, dim=1)                # (B, Cv * 3, H, W)\n",
    "        out = self.output_conv(fused)                     # (B, Cv, H, W)\n",
    "\n",
    "        return out\n",
    "\n",
    "class PMFS(nn.Module):\n",
    "    def __init__(self, n_inputs, in_channels):\n",
    "        super(PMFS, self).__init__()\n",
    "        self.amff_out_channels = [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12][:n_inputs]\n",
    "        self.amff = AMFF(n_inputs = n_inputs, in_channels = in_channels, out_channels = self.amff_out_channels)\n",
    "        \n",
    "        self.attention_in_channels = sum(self.amff_out_channels)\n",
    "        self.pmcs = PMCS(in_channels = self.attention_in_channels)\n",
    "        self.pmss = PMSS(in_channels = self.attention_in_channels, n_branches = len(self.amff_out_channels))\n",
    "        \n",
    "    def forward(self, x_list):\n",
    "        amff_out = self.amff(x_list)\n",
    "        pmcs_out = self.pmcs(amff_out)\n",
    "        # print(pmcs_out.shape)\n",
    "        pmss_out = self.pmss(pmcs_out)\n",
    "        return pmss_out\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=60, out_channels=1, output_size=224, layers_num=4):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        assert 1 <= layers_num <= 4, \"layers_num 必須是 1 到 4 之間\"\n",
    "\n",
    "        self.layers_num = layers_num\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # 全部 4 層的設計：對應 (in → out, size)\n",
    "        channels = [64, 32, 16, 8][-layers_num:]\n",
    "        channels = [in_channels] + channels\n",
    "        # print(channels)\n",
    "        self.all_ups = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[i], channels[i + 1], kernel_size=2, stride=2) for i in range(layers_num)\n",
    "        ])\n",
    "            \n",
    "\n",
    "        self.all_convs = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv2d(channels[i + 1], channels[i + 1], kernel_size=3, padding=1), nn.ReLU(inplace=True)) for i in range(layers_num)\n",
    "        ])\n",
    "\n",
    "        # 根據 layers_num 只保留最後 N 層\n",
    "        self.ups = self.all_ups\n",
    "        self.convs = self.all_convs\n",
    "\n",
    "        # 輸出層根據最後一層 conv 的 output channel 決定\n",
    "        out_ch = 8\n",
    "        self.out_conv = nn.Conv2d(out_ch, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for up, conv in zip(self.ups, self.convs):\n",
    "            # print(\"origin\")\n",
    "            # print(x.shape)\n",
    "            x = up(x)\n",
    "            # print(x.shape)\n",
    "            x = conv(x)\n",
    "            # print(x.shape)\n",
    "\n",
    "        x = F.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "        return self.out_conv(x)\n",
    "        \n",
    "class UpsampleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=32, output_size=224):\n",
    "        super(UpsampleConvBlock, self).__init__()\n",
    "        self.conv = ConvLayer(in_channels, out_channels, kernel=3)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.interpolate(x, size=(self.output_size, self.output_size), mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "        \n",
    "class HybridSegModel(nn.Module):\n",
    "    def __init__(self, in_channels = 1, out_channels = 2, output_size = 224, layers_num = 5):\n",
    "        super(HybridSegModel, self).__init__()\n",
    "\n",
    "        ch_list=[128, 256, 320, 640, 1024]\n",
    "        gr_list=[14, 16, 20, 40, 160]\n",
    "        n_layers=[8, 16, 16, 16, 4]\n",
    "        pool_layer=[1, 0, 1, 1, 0]\n",
    "        \n",
    "        self.layers_num = layers_num\n",
    "        self.backbone = HarDNetBackbone(in_channels, ch_list = ch_list[:layers_num], gr_list = gr_list[:layers_num], n_layers = n_layers[:layers_num], pool_layer = pool_layer[:layers_num])\n",
    "\n",
    "        n_attention = layers_num + 1\n",
    "        pmfs_in_channels = self.backbone.attention_channels\n",
    "        self.pmfs = PMFS(n_inputs = n_attention, in_channels = pmfs_in_channels)\n",
    "\n",
    "        decoder_in_channels = self.pmfs.attention_in_channels\n",
    "        decoder_out_channels = 32\n",
    "        decoder_layers = layers_num - 1\n",
    "        self.decoder = Decoder(in_channels = decoder_in_channels, out_channels = decoder_out_channels, output_size = output_size, layers_num = decoder_layers)\n",
    "        \n",
    "        self.upsample_list = nn.ModuleList([\n",
    "            UpsampleConvBlock(64, out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[0], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[1], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[2], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[3], out_channels=32, output_size=output_size),\n",
    "            UpsampleConvBlock(ch_list[4], out_channels=32, output_size=output_size)\n",
    "        ])\n",
    "\n",
    "        final_in_channels = self.layers_num * 32 + decoder_out_channels\n",
    "        self.final_conv = nn.Conv2d(final_in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        attention_list = self.backbone(x)\n",
    "        attention_upsample = []\n",
    "        for i in range(self.layers_num):\n",
    "            upsample = self.upsample_list[i](attention_list[i])\n",
    "            attention_upsample.append(upsample)\n",
    "        pmfs_out = self.pmfs(attention_list)\n",
    "        out = self.decoder(pmfs_out)\n",
    "\n",
    "        attention_upsample_cat = torch.cat(attention_upsample, axis = 1)\n",
    "        out_cat = torch.cat([attention_upsample_cat, out], axis = 1)\n",
    "\n",
    "        out = self.final_conv(out_cat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "776c9755-b8f5-4802-9b19-e8eb67895bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = HybridSegModel(in_channels = \u001b[32m1\u001b[39m, out_channels = \u001b[32m2\u001b[39m, layers_num = \u001b[32m5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 353\u001b[39m, in \u001b[36mHybridSegModel.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, output_size, layers_num)\u001b[39m\n\u001b[32m    351\u001b[39m decoder_out_channels = \u001b[32m32\u001b[39m\n\u001b[32m    352\u001b[39m decoder_layers = layers_num - \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[38;5;28mself\u001b[39m.decoder = Decoder(in_channels = decoder_in_channels, out_channels = decoder_out_channels, output_size = output_size, layers_num = decoder_layers)\n\u001b[32m    355\u001b[39m \u001b[38;5;28mself\u001b[39m.upsample_list = nn.ModuleList([\n\u001b[32m    356\u001b[39m     UpsampleConvBlock(\u001b[32m64\u001b[39m, out_channels=\u001b[32m32\u001b[39m, output_size=output_size),\n\u001b[32m    357\u001b[39m     UpsampleConvBlock(ch_list[\u001b[32m0\u001b[39m], out_channels=\u001b[32m32\u001b[39m, output_size=output_size),\n\u001b[32m   (...)\u001b[39m\u001b[32m    361\u001b[39m     UpsampleConvBlock(ch_list[\u001b[32m4\u001b[39m], out_channels=\u001b[32m32\u001b[39m, output_size=output_size)\n\u001b[32m    362\u001b[39m ])\n\u001b[32m    364\u001b[39m final_in_channels = \u001b[38;5;28mself\u001b[39m.layers_num * \u001b[32m32\u001b[39m + decoder_out_channels\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 308\u001b[39m, in \u001b[36mDecoder.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, output_size, layers_num)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.convs = \u001b[38;5;28mself\u001b[39m.all_convs\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# 輸出層根據最後一層 conv 的 output channel 決定\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m out_ch = [\u001b[32m64\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m8\u001b[39m][layers_num]\n\u001b[32m    309\u001b[39m \u001b[38;5;28mself\u001b[39m.out_conv = nn.Conv2d(out_ch, out_channels, kernel_size=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "model = HybridSegModel(in_channels = 1, out_channels = 2, layers_num = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9067094f-011a-474a-b2ff-35299297ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b34e2dd1-4647-4af8-b3a6-2872d73b331e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fa12497a-28c9-49f3-bd83-aea14a20b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    print(pred[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9fa0900-696c-4b1d-81e8-d52036d432d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3,534,343\n",
      "Trainable parameters: 3,534,343\n"
     ]
    }
   ],
   "source": [
    "model = HybridSegModel(in_channels = 3, out_channels = 2, layers_num = 3)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "439df8ad-1a71-446c-9c80-f59c0bf1981a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 16, 1, 1], expected input[1, 8, 224, 224] to have 16 channels, but got 8 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m input_image = torch.zeros((\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m))\n\u001b[32m      8\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m output = model(input_image)\n\u001b[32m     12\u001b[39m end_time = time.time()\n\u001b[32m     14\u001b[39m total_time += (end_time - start_time)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 373\u001b[39m, in \u001b[36mHybridSegModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    371\u001b[39m     attention_upsample.append(upsample)\n\u001b[32m    372\u001b[39m pmfs_out = \u001b[38;5;28mself\u001b[39m.pmfs(attention_list)\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m out = \u001b[38;5;28mself\u001b[39m.decoder(pmfs_out)\n\u001b[32m    375\u001b[39m attention_upsample_cat = torch.cat(attention_upsample, axis = \u001b[32m1\u001b[39m)\n\u001b[32m    376\u001b[39m out_cat = torch.cat([attention_upsample_cat, out], axis = \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 321\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[32m    320\u001b[39m x = F.interpolate(x, size=(\u001b[38;5;28mself\u001b[39m.output_size, \u001b[38;5;28mself\u001b[39m.output_size), mode=\u001b[33m'\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m'\u001b[39m, align_corners=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out_conv(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thyroid/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    550\u001b[39m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m.stride, \u001b[38;5;28mself\u001b[39m.padding, \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups\n\u001b[32m    551\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [32, 16, 1, 1], expected input[1, 8, 224, 224] to have 16 channels, but got 8 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import torch\n",
    "total_time = 0\n",
    "frame_num = 200\n",
    "for i in range(frame_num):\n",
    "    input_image = torch.zeros((1, 3, 256, 256))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output = model(input_image)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time += (end_time - start_time)\n",
    "avg_time = total_time / frame_num\n",
    "fps = 1 / avg_time\n",
    "print(f\"Average FPS over {frame_num} images: {fps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77380e-fe28-41f0-a497-7b7efc2477cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thyroid",
   "language": "python",
   "name": "thyroid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
